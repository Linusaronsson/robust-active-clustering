{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rac.experiment_data import ExperimentReader\n",
    "exp_reader = ExperimentReader([\"rand\"])\n",
    "\n",
    "datasets = [\"20newsgroups\", \"cifar10\", \"mnist\", \"cardiotocography\", \"yeast\", \"breast_cancer\", \"ecoli\", \"forest_type_mapping\", \"mushrooms\", \"user_knowledge\"]\n",
    "\n",
    "start_index = 1\n",
    "config = {\n",
    "    \"general_options\": {\n",
    "        \"experiment_name\": \"info_gain_exp\",\n",
    "        \"num_repeats\": 14,\n",
    "        \"n_workers\": 14,\n",
    "        \"local\": True,\n",
    "        \"verbose\": False\n",
    "    },\n",
    "    \"experiment_options\": {\n",
    "        \"seed\": 33,\n",
    "        \"num_feedback\": 0.01,\n",
    "        \"noise_level\": [0.4],\n",
    "        \"persistent_noise_level\": 0.0,\n",
    "        \"save_matrix_data\": False,\n",
    "        \"infer_sims\": False,\n",
    "        \"predict_sims\": False,\n",
    "        \"clustering_alg\": \"CC\",\n",
    "        \"warm_start\": [0, 0.01, 0.02, 0.05]\n",
    "    },\n",
    "\n",
    "    \"sim_init_options\": {\n",
    "        \"K_init\": 10,\n",
    "        \"sim_init\": [0.01],\n",
    "        \"sim_init_type\": [\"zeros\"]\n",
    "    },\n",
    "    \"query_strategy_options\": {\n",
    "        \"acq_fn\": [\"freq\", \"entropy\", \"info_gain_object\", \"cluster_incon\", \"maxexp\"],\n",
    "        \"eps\": 0.3,\n",
    "        \"beta\": 8,\n",
    "        \"tau\": [7],\n",
    "        \"alpha\": 1,\n",
    "        \"num_maxmin_edges\": -1,\n",
    "\n",
    "        \"use_power\": [True],\n",
    "        \"use_grumbel\": [True],\n",
    "        \"power_beta\": [1],\n",
    "        \"sparse_sim_matrix\": [True],\n",
    "        \"mean_field_beta\": [20],\n",
    "        \"info_gain_lambda\": [20],\n",
    "        \"running_avg\": True,\n",
    "        \"num_edges_info_gain\": [50],\n",
    "        \"U_size\": [1],\n",
    "        \"G_size\": [1],\n",
    "        \"info_gain_object_mode\": [\"uniform\"],\n",
    "        \"info_gain_pair_mode\": [\"uniform\", \"entropy\"],\n",
    "        \"mf_iterations\": [50],\n",
    "    },\n",
    "    \"dataset_options\": {\n",
    "        \"dataset\": datasets\n",
    "    }\n",
    "}\n",
    "\n",
    "start_index = exp_reader.generate_experiments(\n",
    "    folder=\"../configs/info_gain_exp/\", \n",
    "    options_to_keep=[],\n",
    "    start_index=start_index,\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15544\\2896419572.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_all_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../experiment_results_local/info_gain_exp/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m ac.generate_AL_curves(\n\u001b[0;32m     62\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\github_projects\\robust-active-clustering\\rac\\experiment_data.py\u001b[0m in \u001b[0;36mread_all_data\u001b[1;34m(self, folder)\u001b[0m\n\u001b[0;32m    160\u001b[0m                         \u001b[0msub_dat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmet_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                     \u001b[0msub_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_dat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_dat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no data found in folder\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    593\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             new_data = concatenate_managers(\n\u001b[0m\u001b[0;32m    596\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mfastpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m             \u001b[0mfastpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[0mupcasted_na\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dtype_to_na_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_none_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m     to_concat = [\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     to_concat = [\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     ]\n",
      "\u001b[1;32mc:\\Users\\linaro\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    503\u001b[0m                     \u001b[1;31m# NB: we should never get here with empty_dtype integer or bool;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                     \u001b[1;31m#  if we did, the missing_arr.fill would cast to gibberish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m                     \u001b[0mmissing_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m                     \u001b[0mmissing_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mmissing_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rac.experiment_data import ExperimentReader\n",
    "metrics = [\"rand\", \"ami\"]\n",
    "ac = ExperimentReader(metrics=metrics)\n",
    "\n",
    "datasets = [\"20newsgroups\", \"cifar10\", \"mnist\", \"cardiotocography\", \"yeast\", \"breast_cancer\", \"ecoli\", \"forest_type_mapping\", \"mushrooms\", \"user_knowledge\"]\n",
    "\n",
    "config = {\n",
    "    \"general_options\": {\n",
    "        \"experiment_name\": \"info_gain_exp\",\n",
    "        \"num_repeats\": 14,\n",
    "        \"n_workers\": 14,\n",
    "        \"local\": True,\n",
    "        \"verbose\": False\n",
    "    },\n",
    "    \"experiment_options\": {\n",
    "        \"seed\": 33,\n",
    "        \"num_feedback\": 0.01,\n",
    "        \"noise_level\": [0.4],\n",
    "        \"persistent_noise_level\": 0.0,\n",
    "        \"save_matrix_data\": False,\n",
    "        \"infer_sims\": False,\n",
    "        \"predict_sims\": False,\n",
    "        \"clustering_alg\": \"CC\",\n",
    "        \"warm_start\": [0, 0.01, 0.02, 0.05]\n",
    "    },\n",
    "\n",
    "    \"sim_init_options\": {\n",
    "        \"K_init\": 10,\n",
    "        \"sim_init\": [0.01],\n",
    "        \"sim_init_type\": [\"zeros\"]\n",
    "    },\n",
    "    \"query_strategy_options\": {\n",
    "        \"acq_fn\": [\"freq\", \"entropy\", \"info_gain_object\", \"cluster_incon\", \"maxexp\"],\n",
    "        \"eps\": 0.3,\n",
    "        \"beta\": 8,\n",
    "        \"tau\": [7],\n",
    "        \"alpha\": 1,\n",
    "        \"num_maxmin_edges\": -1,\n",
    "\n",
    "        \"use_power\": [True],\n",
    "        \"use_grumbel\": [True],\n",
    "        \"power_beta\": [1],\n",
    "        \"sparse_sim_matrix\": [True],\n",
    "        \"mean_field_beta\": [20],\n",
    "        \"info_gain_lambda\": [20],\n",
    "        \"running_avg\": True,\n",
    "        \"num_edges_info_gain\": [50],\n",
    "        \"U_size\": [1],\n",
    "        \"G_size\": [1],\n",
    "        \"info_gain_object_mode\": [\"uniform\"],\n",
    "        \"info_gain_pair_mode\": [\"uniform\", \"entropy\"],\n",
    "        \"mf_iterations\": [50],\n",
    "    },\n",
    "    \"dataset_options\": {\n",
    "        \"dataset\": datasets\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "data = ac.read_all_data(folder=\"../experiment_results_local/info_gain_exp/\")\n",
    "ac.generate_AL_curves(\n",
    "    data,\n",
    "    save_location=\"../plots/info_gain_exp/\",\n",
    "    categorize=[\"dataset\", \"warm_start\", \"use_grumbel\", \"power_beta\", \"info_gain_pair_mode\"],\n",
    "    compare=[\"acq_fn\"],\n",
    "    vary=[\"x\"],\n",
    "    auc=True,\n",
    "    summary_method=\"auc_max_ind\",\n",
    "    indices=[], \n",
    "    threshold=1,\n",
    "    err_style=\"band\",\n",
    "    marker=\"o\",\n",
    "    markersize=6,\n",
    "    capsize=6,\n",
    "    linestyle=\"solid\",\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "def select_objects_info_gain(q, U_size, x, y, mode=\"uniform\"):\n",
    "    N = q.shape[0]\n",
    "    if mode == \"uniform\":\n",
    "        return np.setdiff1d(np.random.choice(N, U_size, replace=False), [x, y])\n",
    "    elif mode == \"entropy\":\n",
    "        # Exclude x and y from the computation\n",
    "        indices = np.arange(N) != x\n",
    "        indices &= np.arange(N) != y\n",
    "\n",
    "        # Compute P(e_ix | Q) and P(e_iy | Q) for all i (except x and y)\n",
    "        P_e_ix_Q = np.sum(q[indices, :] * q[x, :], axis=1)\n",
    "        P_e_iy_Q = np.sum(q[indices, :] * q[y, :], axis=1)\n",
    "\n",
    "        # Compute entropy of P(e_ix | Q) and P(e_iy | Q)\n",
    "        entropy_e_ix_Q = scipy_entropy(np.stack((P_e_ix_Q, 1 - P_e_ix_Q), axis=1))\n",
    "        entropy_e_iy_Q = scipy_entropy(np.stack((P_e_iy_Q, 1 - P_e_iy_Q), axis=1))\n",
    "\n",
    "        # Compute the average entropy\n",
    "        avg_entropy = (entropy_e_ix_Q + entropy_e_iy_Q) / 2\n",
    "\n",
    "        # Rank objects based on average entropy and select top U_size objects\n",
    "        ranked_indices = np.argsort(avg_entropy)[::-1][:U_size]\n",
    "\n",
    "        # Extract the top U_size indices, excluding x and y\n",
    "        top_U_indices = np.arange(N)[indices][ranked_indices]\n",
    "        return np.setdiff1d(top_U_indices, [x, y])\n",
    "    elif mode == \"uniform_varying\":\n",
    "        return np.array([])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode (objects): {}\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy as scipy_entropy\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "import numpy as np\n",
    "def update_mean_fields(q_0, h_0, S, x, y, lmbda, beta, L, U_size, G_size):\n",
    "\n",
    "    q = np.copy(q_0)\n",
    "    h = np.copy(h_0)\n",
    "    q_prev = np.copy(q_0)\n",
    "    N = q.shape[0]\n",
    "\n",
    "    U_size = int(U_size * N)\n",
    "    #G_size = int(G_size * self.ac.N)\n",
    "    #G_size = U_size\n",
    "\n",
    "    # Initialize U^0 as an empty set\n",
    "    U_prev = np.array([])\n",
    "\n",
    "    U_all = np.array([x, y])\n",
    "    #U_t = select_objects_info_gain(mode=\"uniform\", q=q, U_size=U_size, x=x, y=y)\n",
    "    U_t = np.arange(N)\n",
    "\n",
    "    #delta_q = np.zeros(h.shape)\n",
    "    for t in range(1, L + 1):\n",
    "        #if t == 1:\n",
    "        #    h[x, :] += S[x, y] * q_0[y, :] - lmbda * q_0[y, :]\n",
    "        #    h[y, :] += S[y, x] * q_0[x, :] - lmbda * q_0[x, :]\n",
    "        #else:\n",
    "\n",
    "        if t == 1:\n",
    "            h = -np.dot(S, q)\n",
    "            q_prev = np.copy(q)\n",
    "        else:\n",
    "        \n",
    "            #if self.ac.info_gain_object_mode == \"uniform_varying\":\n",
    "            #U_t = np.setdiff1d(np.random.choice(N, U_size, replace=False), [x, y])\n",
    "            #G = np.setdiff1d(np.random.choice(U_t, np.minimum(G_size, len(U_prev)), replace=False), [x, y]).astype(int)\n",
    "            #G = U_prev\n",
    "            G = np.setdiff1d(np.arange(N), [x, y])\n",
    "            G = G.astype(int)\n",
    "\n",
    "            U_all = np.union1d(U_all, U_t).astype(int)\n",
    "\n",
    "            G_xy = np.append(G, [x, y]).astype(int)\n",
    "            ######## h = -np.dot(S, q)\n",
    "            ##### q[[x, y]] = scipy_softmax(-self.ac.mean_field_beta*h[[x, y]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "            q = scipy_softmax(-beta*h, axis=1)\n",
    "            delta_q = (q_prev - q) \n",
    "            print(delta_q)\n",
    "\n",
    "            #h[x, :] += S[x, G].dot(delta_q[G]).reshape(h[x, :].shape)\n",
    "            #h[y, :] += S[y, G].dot(delta_q[G]).reshape(h[y, :].shape)\n",
    "            #h[x, :] += lmbda * (q_prev[y, :] - q[y, :])\n",
    "            #h[y, :] += lmbda * (q_prev[x, :] - q[x, :])\n",
    "\n",
    "            # update for objects in U_t\n",
    "            ####delta_q_xy = (q_prev[G_xy, :] - q[G_xy, :])\n",
    "            h += S.dot(delta_q)\n",
    "\n",
    "\n",
    "\n",
    "            q_prev = np.copy(q)\n",
    "            U_prev = U_t\n",
    "\n",
    "    q[U_all] = scipy_softmax(-beta*h[U_all], axis=1)\n",
    "    return q, U_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mean_fields(q_0, h_0, S, x, y, lmbda, beta, L, U_size, G_size):\n",
    "    h = np.copy(h_0)\n",
    "    q = np.copy(q_0)\n",
    "    q_prev = np.copy(q_0)\n",
    "    N = h.shape[0]\n",
    "\n",
    "    #U_size = int(U_size * N)\n",
    "    #G_size = int(G_size * self.ac.N)\n",
    "    #G_size = U_size\n",
    "\n",
    "    # Initialize U^0 as an empty set\n",
    "    U_prev = np.array([])\n",
    "    U_all = np.array([x, y])\n",
    "    delta_q = np.zeros(h.shape)\n",
    "    #U_t = select_objects_info_gain(mode=\"uniform\", q=q, U_size=U_size, x=x, y=y)\n",
    "    #U_t = np.setdiff1d(np.random.choice(N, U_size, replace=False), [x, y])\n",
    "    U_t = np.setdiff1d(np.arange(N), [x, y])\n",
    "    for t in range(1, L + 1):\n",
    "        if t == 1:\n",
    "            h[x, :] += S[x, y] * q_0[y, :] - lmbda * q_0[y, :]\n",
    "            h[y, :] += S[y, x] * q_0[x, :] - lmbda * q_0[x, :]\n",
    "        else:\n",
    "            #if self.ac.info_gain_object_mode == \"uniform_varying\":\n",
    "            #U_t = np.setdiff1d(np.random.choice(N, U_size, replace=False), [x, y])\n",
    "            #G = np.setdiff1d(np.random.choice(U_prev, np.minimum(G_size, len(U_prev)), replace=False), [x, y]).astype(int)\n",
    "            G = U_prev\n",
    "            G = G.astype(int)\n",
    "            U_all = np.union1d(U_all, U_t).astype(int)\n",
    "\n",
    "            G_xy = np.append(G, [x, y]).astype(int)\n",
    "            q[G_xy] = scipy_softmax(-beta*h[G_xy], axis=1)\n",
    "            #q[[x, y]] = scipy_softmax(-self.ac.mean_field_beta*h[[x, y]], axis=1)\n",
    "            delta_q[G_xy, :] = (q_prev[G_xy, :] - q[G_xy, :])\n",
    "\n",
    "            # update for x and y\n",
    "            h[x, :] += S[x, G].dot(delta_q[G]).reshape(h[x, :].shape)\n",
    "            h[y, :] += S[y, G].dot(delta_q[G]).reshape(h[y, :].shape)\n",
    "            h[x, :] += lmbda * (q_prev[y, :] - q[y, :])\n",
    "            h[y, :] += lmbda * (q_prev[x, :] - q[x, :])\n",
    "\n",
    "            # update for objects in U_t\n",
    "            h[U_t, :] += S[U_t][:, G_xy].dot(delta_q[G_xy])\n",
    "\n",
    "\n",
    "            q_prev = np.copy(q)\n",
    "            U_prev = U_t\n",
    "\n",
    "    q[U_all] = scipy_softmax(-beta*h[U_all], axis=1)\n",
    "    return q, U_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rac.correlation_clustering import max_correlation, max_correlation_dynamic_K\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "from scipy import sparse\n",
    "def mean_field_clustering(S, K, betas, true_labels, max_iter=100, tol=1e-6, noise_level=0.0, is_sparse=False, predicted_labels=None, h=None, q=None):\n",
    "    np.fill_diagonal(S, 0)\n",
    "    N = S.shape[0]\n",
    "    if predicted_labels is None and h is None:\n",
    "        predicted_labels, _ = max_correlation_dynamic_K(S, K, 5)\n",
    "    beta = betas[0]\n",
    "\n",
    "    K = len(np.unique(predicted_labels))\n",
    "\n",
    "    \n",
    "    if h is None:\n",
    "        h = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            cluster_indices = np.where(predicted_labels == k)[0]\n",
    "            for i in range(N):\n",
    "                h[i, k] = S[i, cluster_indices].sum()\n",
    "        \n",
    "        #beta = 50\n",
    "        q = scipy_softmax(beta*h, axis=1)\n",
    "    #print(\"INITIAL Q: \", q)\n",
    "\n",
    "    #n_level = 0.3\n",
    "    #noise = n_level * (np.random.rand(N, K) - 0.5)\n",
    "    #q += noise\n",
    "    #q = np.maximum(q, 0)  # Ensure q stays non-negative\n",
    "    #q /= np.sum(q, axis=1, keepdims=True)  # Re-normalize q\n",
    "\n",
    "    if is_sparse and not sparse.issparse(S):\n",
    "        S = sparse.csr_matrix(S)\n",
    "    \n",
    "    #max_iter = 1000\n",
    "    #betas = [1]\n",
    "    #tol = 1e-10\n",
    "    #old_diff = np.inf\n",
    "    for beta in betas:\n",
    "        for iteration in range(max_iter):\n",
    "            h = -S.dot(q)\n",
    "            #h = -np.dot(S, q)\n",
    "            q_new = scipy_softmax(beta*-h, axis=1)\n",
    "            #print(\"--------\")\n",
    "            \n",
    "            #current_solution = np.argmax(q_new, axis=1)\n",
    "            #current_ari = adjusted_rand_score(current_solution, predicted_labels)\n",
    "            #current_ari2 = adjusted_rand_score(current_solution, true_labels)\n",
    "            #current_ari3 = adjusted_rand_score(predicted_labels, true_labels)\n",
    "            # Check for convergence\n",
    "            diff = np.linalg.norm(q_new - q)\n",
    "            #print(\"iteration: \", iteration, \" diff: \", diff, \" beta: \", beta, \" ari: \", current_ari, \"mf: \", current_ari2, \"local search: \", current_ari3)\n",
    "            #if np.abs(diff - old_diff) < tol:\n",
    "            if diff < tol:\n",
    "                print(f'Converged after {iteration} iterations')\n",
    "                break\n",
    "\n",
    "            #old_diff = diff\n",
    "            q = q_new\n",
    "\n",
    "            # Inject noise\n",
    "            #noise = noise_level * (np.random.rand(N, K) - 0.5)\n",
    "            #q += noise\n",
    "            #q = np.maximum(q, 0)  # Ensure q stays non-negative\n",
    "            #q /= np.sum(q, axis=1, keepdims=True)  # Re-normalize q\n",
    "    return np.argmax(q, axis=1), q, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(n_samples, n_features, n_clusters, gamma, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate a similarity matrix with noise, with specified number of clusters.\n",
    "    Includes diagnostic print statements to investigate the issue with noise injection.\n",
    "\n",
    "    Parameters:\n",
    "    n_samples (int): Number of samples in the dataset.\n",
    "    n_features (int): Number of features in the dataset.\n",
    "    n_clusters (int): Number of clusters (classes) in the dataset.\n",
    "    gamma (float): Probability of noise injection, in the range [0, 1].\n",
    "    random_state (int, optional): Seed for random number generator.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The similarity matrix with noise.\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import make_classification\n",
    "    import numpy as np\n",
    "\n",
    "    # Generate a synthetic dataset with specified number of clusters\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, \n",
    "                               n_clusters_per_class=1, n_classes=n_clusters, \n",
    "                               random_state=random_state, n_informative=n_features, n_redundant=0, n_repeated=0)\n",
    "\n",
    "    # Create the initial similarity matrix based on labels\n",
    "    S = np.where(y[:, None] == y, 1, -1).astype(float)  # Ensure it's a float matrix\n",
    "    np.fill_diagonal(S, 0)\n",
    "\n",
    "    # Print the data type of S for diagnostic purposes\n",
    "\n",
    "    # Inject noise into the similarity matrix\n",
    "    for i in range(n_samples):  # Limiting the loop for diagnostic purposes\n",
    "        for j in range(0, i):\n",
    "            if np.random.rand() < gamma:\n",
    "                noise = np.random.uniform(-1, 1)\n",
    "                S[i, j] = noise\n",
    "                S[j, i] = noise\n",
    "\n",
    "    return S, y\n",
    "\n",
    "# Run the diagnostic version of the function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "n_samples = 100\n",
    "S, true_labels = generate_matrix(n_samples=n_samples, n_features=5, n_clusters=n_clusters, gamma=0.7, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "clust_sol, q, h = mean_field_clustering(\n",
    "    S, n_clusters, betas=[beta],\n",
    "    true_labels=true_labels, max_iter=100, tol=1e-10, noise_level=0.0, \n",
    "    is_sparse=False, predicted_labels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels, _ = max_correlation_dynamic_K(S, n_clusters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4182636925332496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "current_ari = adjusted_rand_score(clust_sol, true_labels)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4841764841764842\n"
     ]
    }
   ],
   "source": [
    "current_ari = adjusted_rand_score(predicted_labels, true_labels)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6121424779388329\n"
     ]
    }
   ],
   "source": [
    "current_ari = adjusted_rand_score(predicted_labels, clust_sol)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "x = 1\n",
    "y = 5\n",
    "xi = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_val = S[x, y]\n",
    "S[x, y] = xi\n",
    "S[y, x] = xi\n",
    "clust_sol_new, q_new, h_new = mean_field_clustering(\n",
    "    S, n_clusters, betas=[beta],\n",
    "    true_labels=true_labels, max_iter=100, tol=1e-10, noise_level=0.0, \n",
    "    is_sparse=True, predicted_labels=None, h=h, q=q\n",
    ")\n",
    "S[x, y] = old_val\n",
    "S[y, x] = old_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049240990838687\n"
     ]
    }
   ],
   "source": [
    "current_ari = adjusted_rand_score(clust_sol, clust_sol_new)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_size = 1\n",
    "G_size = 1\n",
    "S = sparse.csr_matrix(S)\n",
    "q_new_fast, U = update_mean_fields(q, h, S, x, y, xi, beta, 100, U_size, G_size)\n",
    "clust_sol_fast = np.argmax(q_new_fast, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626609965936589\n"
     ]
    }
   ],
   "source": [
    "current_ari = adjusted_rand_score(clust_sol_new, clust_sol_fast)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8185350832172447\n"
     ]
    }
   ],
   "source": [
    "current_ari = adjusted_rand_score(clust_sol, clust_sol_fast)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  3,  9,  6, 11,  2,  2, 12,  1,  9,  9, 10,  8,  9,  8,  9,  5,\n",
       "        8,  3,  5,  2, 10,  2,  8, 12,  9,  9,  4,  8,  1, 10,  8,  2,  5,\n",
       "       12, 12,  1,  2, 12,  9,  8,  9,  3,  3,  3, 12, 12,  5,  4, 11,  9,\n",
       "        9,  2,  4,  5,  4,  4,  1, 10,  2,  1,  8,  2, 10, 10,  5,  3,  3,\n",
       "        9,  3,  1,  5,  9, 10,  8,  8, 12,  1,  5,  2, 11,  8,  5,  3, 10,\n",
       "        8,  9,  9, 10, 10,  8,  8, 12, 12,  5, 11, 11, 10,  4,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_sol_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 10\n",
    "h = np.random.rand(n_samples, n_clusters)\n",
    "q = scipy_softmax(-beta*h, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 50\n",
    "y = 210\n",
    "xi = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 14 iterations\n"
     ]
    }
   ],
   "source": [
    "old_val = S[x, y]\n",
    "S[x, y] = xi\n",
    "S[y, x] = xi\n",
    "clust_sol_new, q_new, h_new = mean_field_clustering(\n",
    "    S, n_clusters, betas=[beta],\n",
    "    true_labels=true_labels, max_iter=100, tol=1e-10, noise_level=0.0, \n",
    "    is_sparse=False, predicted_labels=None, h=h, q=q\n",
    ")\n",
    "S[x, y] = old_val\n",
    "S[y, x] = old_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "current_ari = adjusted_rand_score(clust_sol_new, true_labels)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_size = 1\n",
    "G_size = 1\n",
    "q_new_fast, U = update_mean_fields(q, h, S, x, y, xi, beta, 100, U_size, G_size)\n",
    "clust_sol_fast = np.argmax(q_new_fast, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1003578075282969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "current_ari = adjusted_rand_score(clust_sol_fast, true_labels)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1003578075282969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "current_ari = adjusted_rand_score(clust_sol_fast, clust_sol_new)\n",
    "print(current_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
