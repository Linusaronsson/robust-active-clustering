{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from rac.pred_models import CustomTensorDataset, ACCNet\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../datasets/cifar10_original_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#trainset.data = trainset.data\n",
    "#trainset.targets = trainset.targets\n",
    "X_train = trainset.data\n",
    "y_train = trainset.targets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../datasets/cifar10_original_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, num_workers=8)\n",
    "\n",
    "X_test = testset.data\n",
    "y_test = testset.targets\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.load(\"X_train_cifar.npy\")\n",
    "y_train = np.load(\"Y_train_cifar.npy\")\n",
    "X_test = np.load(\"X_test_cifar.npy\")\n",
    "y_test = np.load(\"Y_test_cifar.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, Y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=10,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=None,\n",
    "    flip_y=0,\n",
    "    class_sep=1.0,\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.load(\"../datasets/cifar10_data/cifar10_embedding.npy\")\n",
    "#Y = np.load(\"../datasets/cifar10_data/cifar10_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#X = preprocessing.StandardScaler().fit_transform(X)\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=100)\n",
    "#X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_from_clustering_solution(clustering_solution):\n",
    "    num_clusters = np.max(clustering_solution) + 1\n",
    "    clustering = [[] for _ in range(num_clusters)]\n",
    "    for i in range(len(clustering_solution)):\n",
    "        clustering[clustering_solution[i]].append(i)\n",
    "    return clustering, num_clusters\n",
    "\n",
    "def sim_matrix_from_clustering(clustering, N):\n",
    "    pairwise_similarities = -np.ones((N, N))\n",
    "    for cind in clustering:\n",
    "        pairwise_similarities[np.ix_(cind, cind)] = 1\n",
    "    return pairwise_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sol = clustering_from_clustering_solution(y_train[:10000])\n",
    "train_sim_matrix = sim_matrix_from_clustering(train_sol[0], len(y_train[:10000]))\n",
    "\n",
    "test_sol = clustering_from_clustering_solution(y_test[:10000])\n",
    "test_sim_matrix = sim_matrix_from_clustering(test_sol[0], len(y_test[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sim_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12716\\2120360980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_sim_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sim_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "train_sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(prop_pos, prop_neg, sim_matrix, data):\n",
    "    N = sim_matrix.shape[0]\n",
    "    lower_triangle_indices = np.tril_indices(N, -1)\n",
    "    ind_pos = np.where(sim_matrix[lower_triangle_indices] == 1)[0]\n",
    "    ind_neg = np.where(sim_matrix[lower_triangle_indices] == -1)[0]\n",
    "    num_pos = int(len(ind_pos)*prop_pos)\n",
    "    num_neg = int(len(ind_neg)*prop_neg)\n",
    "    print(\"num_pos: \", num_pos)\n",
    "    print(\"num_neg: \", num_neg)\n",
    "    ind_pos = np.random.choice(ind_pos, num_pos)\n",
    "    ind_neg = np.random.choice(ind_neg, num_neg)\n",
    "    if num_pos < num_neg:\n",
    "        indices = np.concatenate([ind_neg, ind_pos])\n",
    "    else:\n",
    "        indices = np.concatenate([ind_pos, ind_neg])\n",
    "    ind1, ind2 = lower_triangle_indices[0][indices], lower_triangle_indices[1][indices]\n",
    "    x1 = data[ind1]\n",
    "    x2 = data[ind2]\n",
    "    y = sim_matrix[ind1, ind2]\n",
    "    lab1 = np.where(y >= 0)\n",
    "    lab2 = np.where(y < 0)\n",
    "    y[lab1] = 1.0\n",
    "    y[lab2] = 0.0\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pos:  4998\n",
      "num_neg:  22498\n",
      "num_pos:  199800\n",
      "num_neg:  1800000\n"
     ]
    }
   ],
   "source": [
    "x1_train, x2_train, y_train_pairs = get_pairs(0.001, 0.0005, train_sim_matrix, X_train)\n",
    "x1_test, x2_test, y_test_pairs = get_pairs(0.04, 0.04, test_sim_matrix, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_training_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "cifar_test_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CustomTensorDataset(x1_train, x2_train, y_train_pairs, train=True, transform=cifar_training_transform)\n",
    "test_dataset = CustomTensorDataset(x1_test, x2_test, y_test_pairs, transform=cifar_test_transform)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTensorDataset(torch.Tensor(x1_train), torch.Tensor(x2_train), torch.Tensor(y_train_pairs), train=True, transform=None)\n",
    "test_dataset = CustomTensorDataset(torch.Tensor(x1_test), torch.Tensor(x2_test), torch.Tensor(y_test_pairs), transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#class_sample_count = [10, 1, 20, 3, 4] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "class_sample_count = np.unique(y_train_pairs, return_counts=True)[1].tolist()\n",
    "#print(class_sample_count)\n",
    "weights = 1/torch.Tensor(class_sample_count)\n",
    "weights = weights[y_train_pairs]\n",
    "#print(weights)\n",
    "#sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1024)\n",
    "#trainloader = data_utils.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x1, x2, y in train_loader:\n",
    "    #print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44996"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ACCNet(base_net=\"cifarnet\", siamese=True, input_dim=10, p=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "44996\n",
      "0\n",
      "loss:  0.6626848658322542\n",
      "1\n",
      "loss:  0.6166964284556027\n",
      "2\n",
      "loss:  0.5972416568444026\n",
      "3\n",
      "loss:  0.5801654201826051\n",
      "4\n",
      "loss:  0.5654201030692736\n",
      "5\n",
      "loss:  0.5517418956643558\n",
      "6\n",
      "loss:  0.5373497714973399\n",
      "7\n",
      "loss:  0.5202180268058503\n",
      "8\n",
      "loss:  0.50347996023516\n",
      "9\n",
      "loss:  0.48128227557862213\n",
      "10\n",
      "loss:  0.4653147845666849\n",
      "11\n",
      "loss:  0.44859882368959714\n",
      "12\n",
      "loss:  0.4301801134778721\n",
      "13\n",
      "loss:  0.41464201048937044\n",
      "14\n",
      "loss:  0.39964581242170183\n",
      "15\n",
      "loss:  0.386231271570988\n",
      "16\n",
      "loss:  0.36911717423159557\n",
      "17\n",
      "loss:  0.35301996808007263\n",
      "18\n",
      "loss:  0.3385811660021027\n",
      "19\n",
      "loss:  0.32352454017517723\n",
      "20\n",
      "loss:  0.30998434225264704\n",
      "21\n",
      "loss:  0.29868366711597727\n",
      "22\n",
      "loss:  0.2839349297141492\n",
      "23\n",
      "loss:  0.27280433918828256\n",
      "24\n",
      "loss:  0.2629156900299814\n",
      "25\n",
      "loss:  0.2523943303157125\n",
      "26\n",
      "loss:  0.23951188185545902\n",
      "27\n",
      "loss:  0.2307989171780959\n",
      "28\n",
      "loss:  0.2181866315139998\n",
      "29\n",
      "loss:  0.21215747028875867\n",
      "30\n",
      "loss:  0.20340026644798284\n",
      "31\n",
      "loss:  0.19684385044889322\n",
      "32\n",
      "loss:  0.1850030096375341\n",
      "33\n",
      "loss:  0.17799917136658963\n",
      "34\n",
      "loss:  0.16997857991541115\n",
      "35\n",
      "loss:  0.16343305128146338\n",
      "36\n",
      "loss:  0.15553500227371642\n",
      "37\n",
      "loss:  0.14601778414638777\n",
      "38\n",
      "loss:  0.14221560494252003\n",
      "39\n",
      "loss:  0.13619575292954225\n",
      "40\n",
      "loss:  0.13040133080438573\n",
      "41\n",
      "loss:  0.12775607159059\n",
      "42\n",
      "loss:  0.1210500089988118\n",
      "43\n",
      "loss:  0.11504366559164109\n",
      "44\n",
      "loss:  0.11243791767527228\n",
      "45\n",
      "loss:  0.11101638555091632\n",
      "46\n",
      "loss:  0.10541290123751919\n",
      "47\n",
      "loss:  0.09958422385216836\n",
      "48\n",
      "loss:  0.09857133431702593\n",
      "49\n",
      "loss:  0.09481428450870712\n",
      "50\n",
      "loss:  0.09291030937991643\n",
      "51\n",
      "loss:  0.09041898696460544\n",
      "52\n",
      "loss:  0.0869403596616235\n",
      "53\n",
      "loss:  0.08137922680234606\n",
      "54\n",
      "loss:  0.08101343280747623\n",
      "55\n",
      "loss:  0.07898334864836229\n",
      "56\n",
      "loss:  0.07675568369359798\n",
      "57\n",
      "loss:  0.0765376143592155\n",
      "58\n",
      "loss:  0.07068152164323084\n",
      "59\n",
      "loss:  0.06967328528807384\n",
      "60\n",
      "loss:  0.06765081404725423\n",
      "61\n",
      "loss:  0.0669457555554374\n",
      "62\n",
      "loss:  0.06599731413145532\n",
      "63\n",
      "loss:  0.0616402968364505\n",
      "64\n",
      "loss:  0.0619575522171052\n",
      "65\n",
      "loss:  0.06096988193188108\n",
      "66\n",
      "loss:  0.0594043958255667\n",
      "67\n",
      "loss:  0.054869498574950985\n",
      "68\n",
      "loss:  0.0546335319363879\n",
      "69\n",
      "loss:  0.051884530249994947\n",
      "70\n",
      "loss:  0.05390724447629829\n",
      "71\n",
      "loss:  0.04973959617240493\n",
      "72\n",
      "loss:  0.05173625262765408\n",
      "73\n",
      "loss:  0.05018457785342511\n",
      "74\n",
      "loss:  0.049160404290803594\n",
      "75\n",
      "loss:  0.04792419890089194\n",
      "76\n",
      "loss:  0.047995210234692484\n",
      "77\n",
      "loss:  0.04449452568597365\n",
      "78\n",
      "loss:  0.046022961609636655\n",
      "79\n",
      "loss:  0.04303588409862979\n",
      "80\n",
      "loss:  0.04409254585129234\n",
      "81\n",
      "loss:  0.04284465140615919\n",
      "82\n",
      "loss:  0.04284271370766593\n",
      "83\n",
      "loss:  0.03963968326719156\n",
      "84\n",
      "loss:  0.03955314170672394\n",
      "85\n",
      "loss:  0.037464823440564766\n",
      "86\n",
      "loss:  0.03839103783747214\n",
      "87\n",
      "loss:  0.038176621759603506\n",
      "88\n",
      "loss:  0.038158098944039175\n",
      "89\n",
      "loss:  0.03557444996432283\n",
      "90\n",
      "loss:  0.036726027855317735\n",
      "91\n",
      "loss:  0.03573410908944753\n",
      "92\n",
      "loss:  0.03503385411944306\n",
      "93\n",
      "loss:  0.03382323880046122\n",
      "94\n",
      "loss:  0.034003528835242394\n",
      "95\n",
      "loss:  0.03331497418711865\n",
      "96\n",
      "loss:  0.03239774047494691\n",
      "97\n",
      "loss:  0.03193206624222908\n",
      "98\n",
      "loss:  0.03279580971135459\n",
      "99\n",
      "loss:  0.03222672768081459\n",
      "100\n",
      "loss:  0.03125211963790514\n",
      "101\n",
      "loss:  0.029486793336440158\n",
      "102\n",
      "loss:  0.03186170415314708\n",
      "103\n",
      "loss:  0.03008092367254755\n",
      "104\n",
      "loss:  0.029117676517635603\n",
      "105\n",
      "loss:  0.027997135058979702\n",
      "106\n",
      "loss:  0.02848491046646916\n",
      "107\n",
      "loss:  0.028103994552578965\n",
      "108\n",
      "loss:  0.02843771657033564\n",
      "109\n",
      "loss:  0.027307525374313035\n",
      "110\n",
      "loss:  0.02746096076394664\n",
      "111\n",
      "loss:  0.027450680949535573\n",
      "112\n",
      "loss:  0.02615158569686498\n",
      "113\n",
      "loss:  0.02476294038723987\n",
      "114\n",
      "loss:  0.024459916203128297\n",
      "115\n",
      "loss:  0.02591581408525141\n",
      "116\n",
      "loss:  0.025877062192708723\n",
      "117\n",
      "loss:  0.02528207592487193\n",
      "118\n",
      "loss:  0.025619108819882563\n",
      "119\n",
      "loss:  0.024444562923808155\n",
      "120\n",
      "loss:  0.02505419925751673\n",
      "121\n",
      "loss:  0.023059774231656897\n",
      "122\n",
      "loss:  0.02358441592717658\n",
      "123\n",
      "loss:  0.02351496723647086\n",
      "124\n",
      "loss:  0.021937555461287315\n",
      "125\n",
      "loss:  0.022093917742583475\n",
      "126\n",
      "loss:  0.024097648931511978\n",
      "127\n",
      "loss:  0.022253730861541473\n",
      "128\n",
      "loss:  0.022740406492328512\n",
      "129\n",
      "loss:  0.02179765104051172\n",
      "130\n",
      "loss:  0.02281599482276746\n",
      "131\n",
      "loss:  0.021849938252188072\n",
      "132\n",
      "loss:  0.02197004376597472\n",
      "133\n",
      "loss:  0.02224209897790185\n",
      "134\n",
      "loss:  0.02063518475920738\n",
      "135\n",
      "loss:  0.021855303341040022\n",
      "136\n",
      "loss:  0.019677865220568424\n",
      "137\n",
      "loss:  0.020490533819820873\n",
      "138\n",
      "loss:  0.02028422465438119\n",
      "139\n",
      "loss:  0.020401581487804698\n",
      "140\n",
      "loss:  0.02091050262372322\n",
      "141\n",
      "loss:  0.019411734444074666\n",
      "142\n",
      "loss:  0.019905265843370552\n",
      "143\n",
      "loss:  0.01963516517734305\n",
      "144\n",
      "loss:  0.019684440428581532\n",
      "145\n",
      "loss:  0.018672534827339455\n",
      "146\n",
      "loss:  0.018375390096516335\n",
      "147\n",
      "loss:  0.01968066519500668\n",
      "148\n",
      "loss:  0.02008966112496273\n",
      "149\n",
      "loss:  0.019154416006305342\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.0005, momentum=0.9, weight_decay=5e-4)\n",
    "print(\"training...\")\n",
    "print(len(train_dataset))\n",
    "net.train()\n",
    "for epoch in range(150):  # loop over the dataset multiple times\n",
    "    print(epoch)\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        #print(np.unique(data[2], return_counts=True))\n",
    "        #print(data[2])\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(x1, x2)\n",
    "        outputs = outputs.reshape((outputs.shape[0]))\n",
    "        #labels = labels.reshape((labels.shape[0], 1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "    print(\"loss: \", running_loss/step)\n",
    "    step = 0\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/tmp.1460525/ipykernel_612357/1410191718.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /dev/shm/PyTorch/1.12.0/foss-2022a-CUDA-11.7.0/pytorch/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  prob = torch.Tensor([1-pred, pred]).T.to(device)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy as scipy_entropy\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "probs = torch.zeros([len(test_dataset), 2]).to(device)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    evaluated_instances = 0\n",
    "    for data in test_loader:\n",
    "        x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(x1, x2)\n",
    "        pred = nn.Sigmoid()(outputs)\n",
    "        #print(pred)\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred = pred.reshape(pred.shape[0])\n",
    "        prob = torch.Tensor([1-pred, pred]).T.to(device)\n",
    "        start_slice = evaluated_instances\n",
    "        end_slice = start_slice + x1.shape[0]\n",
    "        probs[start_slice:end_slice] = prob\n",
    "        evaluated_instances = end_slice\n",
    "        #entropy = scipy_entropy(prob)\n",
    "        #pred[pred >= 0.5] = 1\n",
    "        #pred[pred < 0.5] = 0\n",
    "        #print(torch.max(pred.data, 1)[1])\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        #_, predicted = torch.max(pred.data, 1)\n",
    "probs = probs.cpu().numpy()\n",
    "preds = np.argmax(probs, axis=1)\n",
    "entropys = scipy_entropy(probs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is on right device and is in TRAIN mode.\n",
    "# Train mode is needed to activate randomness in dropout modules.\n",
    "n_drop = 10\n",
    "net.train()\n",
    "# Create a tensor to hold probabilities\n",
    "num_classes = 2\n",
    "probs_mc = torch.zeros([len(test_dataset), 2]).to(device)\n",
    "# Create a dataloader object to load the dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Repeat n_drop number of times to obtain n_drop dropout samples per data instance\n",
    "    for i in range(n_drop):\n",
    "        evaluated_instances = 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            # Calculate softmax (probabilities) of predictions\n",
    "            x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "            outputs = net(x1, x2)\n",
    "            pred = nn.Sigmoid()(outputs)\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred = pred.reshape(pred.shape[0])\n",
    "            prob = torch.Tensor([1-pred, pred]).T.to(device)\n",
    "        \n",
    "            # Accumulate the calculated batch of probabilities into the tensor to return\n",
    "            start_slice = evaluated_instances\n",
    "            end_slice = start_slice + x1.shape[0]\n",
    "            probs_mc[start_slice:end_slice] += prob\n",
    "            evaluated_instances = end_slice\n",
    "\n",
    "# Divide through by n_drop to get average prob.\n",
    "probs_mc /= n_drop\n",
    "probs_mc = probs_mc.cpu().numpy()\n",
    "preds_mc = np.argmax(probs_mc, axis=1)\n",
    "entropys_mc = scipy_entropy(probs_mc.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = 0.01\n",
    "threshold = 0.000001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(entropys <= threshold)[0]\n",
    "preds_new = preds[inds]\n",
    "y_test_pairs_new = y_test_pairs[inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropys_mc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mentropys_mc\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m preds_new_mc \u001b[38;5;241m=\u001b[39m preds_mc[inds]\n\u001b[1;32m      3\u001b[0m y_test_pairs_new_mc \u001b[38;5;241m=\u001b[39m y_test_pairs[inds]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entropys_mc' is not defined"
     ]
    }
   ],
   "source": [
    "inds = np.where(entropys_mc <= threshold)[0]\n",
    "preds_new_mc = preds_mc[inds]\n",
    "y_test_pairs_new_mc = y_test_pairs[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99    782425\n",
      "         1.0       0.84      0.63      0.72     23562\n",
      "\n",
      "    accuracy                           0.99    805987\n",
      "   macro avg       0.92      0.81      0.86    805987\n",
      "weighted avg       0.98      0.99      0.98    805987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_pairs_new, preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      4182\n",
      "         1.0       1.00      1.00      1.00       220\n",
      "\n",
      "    accuracy                           1.00      4402\n",
      "   macro avg       1.00      1.00      1.00      4402\n",
      "weighted avg       1.00      1.00      1.00      4402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_pairs_new_mc, preds_new_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
