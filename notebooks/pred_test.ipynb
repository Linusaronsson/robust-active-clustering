{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from rac.pred_models import CustomTensorDataset, ACCNet\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../datasets/cifar10_original_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#trainset.data = trainset.data\n",
    "#trainset.targets = trainset.targets\n",
    "X_train = trainset.data\n",
    "y_train = trainset.targets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../datasets/cifar10_original_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024, num_workers=8)\n",
    "\n",
    "X_test = testset.data\n",
    "y_test = testset.targets\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.load(\"X_train_cifar.npy\")\n",
    "y_train = np.load(\"Y_train_cifar.npy\")\n",
    "X_test = np.load(\"X_test_cifar.npy\")\n",
    "y_test = np.load(\"Y_test_cifar.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, Y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=10,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=None,\n",
    "    flip_y=0,\n",
    "    class_sep=1.0,\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.load(\"../datasets/cifar10_data/cifar10_embedding.npy\")\n",
    "#Y = np.load(\"../datasets/cifar10_data/cifar10_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#X = preprocessing.StandardScaler().fit_transform(X)\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=100)\n",
    "#X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_from_clustering_solution(clustering_solution):\n",
    "    num_clusters = np.max(clustering_solution) + 1\n",
    "    clustering = [[] for _ in range(num_clusters)]\n",
    "    for i in range(len(clustering_solution)):\n",
    "        clustering[clustering_solution[i]].append(i)\n",
    "    return clustering, num_clusters\n",
    "\n",
    "def sim_matrix_from_clustering(clustering, N):\n",
    "    pairwise_similarities = -np.ones((N, N))\n",
    "    for cind in clustering:\n",
    "        pairwise_similarities[np.ix_(cind, cind)] = 1\n",
    "    return pairwise_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sol = clustering_from_clustering_solution(y_train[:10000])\n",
    "train_sim_matrix = sim_matrix_from_clustering(train_sol[0], len(y_train[:10000]))\n",
    "\n",
    "test_sol = clustering_from_clustering_solution(y_test[:10000])\n",
    "test_sim_matrix = sim_matrix_from_clustering(test_sol[0], len(y_test[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4000)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(prop_pos, prop_neg, sim_matrix, data):\n",
    "    N = sim_matrix.shape[0]\n",
    "    lower_triangle_indices = np.tril_indices(N, -1)\n",
    "    ind_pos = np.where(sim_matrix[lower_triangle_indices] == 1)[0]\n",
    "    ind_neg = np.where(sim_matrix[lower_triangle_indices] == -1)[0]\n",
    "    num_pos = int(len(ind_pos)*prop_pos)\n",
    "    num_neg = int(len(ind_neg)*prop_neg)\n",
    "    print(\"num_pos: \", num_pos)\n",
    "    print(\"num_neg: \", num_neg)\n",
    "    ind_pos = np.random.choice(ind_pos, num_pos)\n",
    "    ind_neg = np.random.choice(ind_neg, num_neg)\n",
    "    if num_pos < num_neg:\n",
    "        indices = np.concatenate([ind_neg, ind_pos])\n",
    "    else:\n",
    "        indices = np.concatenate([ind_pos, ind_neg])\n",
    "    ind1, ind2 = lower_triangle_indices[0][indices], lower_triangle_indices[1][indices]\n",
    "    x1 = data[ind1]\n",
    "    x2 = data[ind2]\n",
    "    y = sim_matrix[ind1, ind2]\n",
    "    lab1 = np.where(y >= 0)\n",
    "    lab2 = np.where(y < 0)\n",
    "    y[lab1] = 1.0\n",
    "    y[lab2] = 0.0\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pos:  3991\n",
      "num_neg:  35998\n",
      "num_pos:  1994\n",
      "num_neg:  17985\n"
     ]
    }
   ],
   "source": [
    "x1_train, x2_train, y_train_pairs = get_pairs(0.005, 0.005, train_sim_matrix, X_train)\n",
    "x1_test, x2_test, y_test_pairs = get_pairs(0.04, 0.04, test_sim_matrix, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_training_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "cifar_test_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CustomTensorDataset(x1_train, x2_train, y_train_pairs, train=True, transform=cifar_training_transform)\n",
    "test_dataset = CustomTensorDataset(x1_test, x2_test, y_test_pairs, transform=cifar_test_transform)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTensorDataset(torch.Tensor(x1_train), torch.Tensor(x2_train), torch.Tensor(y_train_pairs), train=True, transform=None)\n",
    "test_dataset = CustomTensorDataset(torch.Tensor(x1_test), torch.Tensor(x2_test), torch.Tensor(y_test_pairs), transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#class_sample_count = [10, 1, 20, 3, 4] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "class_sample_count = np.unique(y_train_pairs, return_counts=True)[1].tolist()\n",
    "#print(class_sample_count)\n",
    "weights = 1/torch.Tensor(class_sample_count)\n",
    "weights = weights[y_train_pairs]\n",
    "#print(weights)\n",
    "#sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1024)\n",
    "#trainloader = data_utils.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x1, x2, y in train_loader:\n",
    "    #print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71996"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ACCNet(base_net=\"three_layer_net\", siamese=True, input_dim=10, p=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "71996\n",
      "0\n",
      "loss:  0.6439411264591747\n",
      "1\n",
      "loss:  0.4942533004548815\n",
      "2\n",
      "loss:  0.36534165697958737\n",
      "3\n",
      "loss:  0.27353165545976826\n",
      "4\n",
      "loss:  0.21810425380244852\n",
      "5\n",
      "loss:  0.17567392035796203\n",
      "6\n",
      "loss:  0.14667808479802785\n",
      "7\n",
      "loss:  0.1244002818390727\n",
      "8\n",
      "loss:  0.10896450776987088\n",
      "9\n",
      "loss:  0.09496620143197167\n",
      "10\n",
      "loss:  0.08579845904098733\n",
      "11\n",
      "loss:  0.0781878107434669\n",
      "12\n",
      "loss:  0.07047317224656904\n",
      "13\n",
      "loss:  0.06480762602030882\n",
      "14\n",
      "loss:  0.061026785379657264\n",
      "15\n",
      "loss:  0.055864155458131184\n",
      "16\n",
      "loss:  0.052307698093522856\n",
      "17\n",
      "loss:  0.04817258003979643\n",
      "18\n",
      "loss:  0.046511807226831375\n",
      "19\n",
      "loss:  0.04513697671993739\n",
      "20\n",
      "loss:  0.0424493223636285\n",
      "21\n",
      "loss:  0.0395495519141627\n",
      "22\n",
      "loss:  0.037830579976161564\n",
      "23\n",
      "loss:  0.035706712724073844\n",
      "24\n",
      "loss:  0.03339906605039343\n",
      "25\n",
      "loss:  0.032666830238965806\n",
      "26\n",
      "loss:  0.03101576941893331\n",
      "27\n",
      "loss:  0.031044589327617965\n",
      "28\n",
      "loss:  0.028582480365423383\n",
      "29\n",
      "loss:  0.02704673020204614\n",
      "30\n",
      "loss:  0.027018757194244042\n",
      "31\n",
      "loss:  0.026801318595744306\n",
      "32\n",
      "loss:  0.025270354269596434\n",
      "33\n",
      "loss:  0.023292701985275725\n",
      "34\n",
      "loss:  0.024113212446314758\n",
      "35\n",
      "loss:  0.0241536528049718\n",
      "36\n",
      "loss:  0.022295747743536088\n",
      "37\n",
      "loss:  0.021874385745327483\n",
      "38\n",
      "loss:  0.02045913927469069\n",
      "39\n",
      "loss:  0.02043613977168328\n",
      "40\n",
      "loss:  0.020354710547918205\n",
      "41\n",
      "loss:  0.018832874907376815\n",
      "42\n",
      "loss:  0.018346542997802973\n",
      "43\n",
      "loss:  0.01844420860866679\n",
      "44\n",
      "loss:  0.018175310903357007\n",
      "45\n",
      "loss:  0.01782068422682068\n",
      "46\n",
      "loss:  0.016570611181206698\n",
      "47\n",
      "loss:  0.016566413075232832\n",
      "48\n",
      "loss:  0.017009853541274954\n",
      "49\n",
      "loss:  0.015434928528485924\n",
      "50\n",
      "loss:  0.015866719703804252\n",
      "51\n",
      "loss:  0.015522007645864323\n",
      "52\n",
      "loss:  0.014518243113432402\n",
      "53\n",
      "loss:  0.015472397113453452\n",
      "54\n",
      "loss:  0.014905568193179054\n",
      "55\n",
      "loss:  0.014208783049440399\n",
      "56\n",
      "loss:  0.014382209198169455\n",
      "57\n",
      "loss:  0.01392018976241708\n",
      "58\n",
      "loss:  0.012850738606645781\n",
      "59\n",
      "loss:  0.013006270445310216\n",
      "60\n",
      "loss:  0.013227333609827597\n",
      "61\n",
      "loss:  0.012981683777334435\n",
      "62\n",
      "loss:  0.01328381090383669\n",
      "63\n",
      "loss:  0.012305995542108171\n",
      "64\n",
      "loss:  0.013253564927022548\n",
      "65\n",
      "loss:  0.012593221598871772\n",
      "66\n",
      "loss:  0.012508682193266655\n",
      "67\n",
      "loss:  0.012145411912142492\n",
      "68\n",
      "loss:  0.012388797579546628\n",
      "69\n",
      "loss:  0.0104867852239441\n",
      "70\n",
      "loss:  0.012778377051821457\n",
      "71\n",
      "loss:  0.011490766927662157\n",
      "72\n",
      "loss:  0.011637879034962316\n",
      "73\n",
      "loss:  0.0106832910055446\n",
      "74\n",
      "loss:  0.011436506275761144\n",
      "75\n",
      "loss:  0.011279597262437548\n",
      "76\n",
      "loss:  0.011068951842313506\n",
      "77\n",
      "loss:  0.009959771341619898\n",
      "78\n",
      "loss:  0.011170045162671102\n",
      "79\n",
      "loss:  0.010269951225555473\n",
      "80\n",
      "loss:  0.01061018521610288\n",
      "81\n",
      "loss:  0.010408140972894641\n",
      "82\n",
      "loss:  0.01036448071126988\n",
      "83\n",
      "loss:  0.01036605019847634\n",
      "84\n",
      "loss:  0.009449337750087444\n",
      "85\n",
      "loss:  0.010426632559134195\n",
      "86\n",
      "loss:  0.009162491714750554\n",
      "87\n",
      "loss:  0.009473885222488206\n",
      "88\n",
      "loss:  0.010527807536917812\n",
      "89\n",
      "loss:  0.009927483342216192\n",
      "90\n",
      "loss:  0.00964959841045995\n",
      "91\n",
      "loss:  0.008729919784580539\n",
      "92\n",
      "loss:  0.009341909696193479\n",
      "93\n",
      "loss:  0.008551716495928672\n",
      "94\n",
      "loss:  0.008909029369561549\n",
      "95\n",
      "loss:  0.008114914521660728\n",
      "96\n",
      "loss:  0.008424799888998375\n",
      "97\n",
      "loss:  0.008834023358543283\n",
      "98\n",
      "loss:  0.009947454526111022\n",
      "99\n",
      "loss:  0.009283930098918057\n",
      "100\n",
      "loss:  0.009067389859921489\n",
      "101\n",
      "loss:  0.008954302472325777\n",
      "102\n",
      "loss:  0.007349808000611005\n",
      "103\n",
      "loss:  0.008481328107763052\n",
      "104\n",
      "loss:  0.008037652442632331\n",
      "105\n",
      "loss:  0.00757100608030084\n",
      "106\n",
      "loss:  0.008406864646060979\n",
      "107\n",
      "loss:  0.007555037927897896\n",
      "108\n",
      "loss:  0.008960228828932125\n",
      "109\n",
      "loss:  0.008003245516205924\n",
      "110\n",
      "loss:  0.007896459235471323\n",
      "111\n",
      "loss:  0.00838732093517668\n",
      "112\n",
      "loss:  0.007639094003727084\n",
      "113\n",
      "loss:  0.008575240432179693\n",
      "114\n",
      "loss:  0.00822366242092923\n",
      "115\n",
      "loss:  0.007092983552792611\n",
      "116\n",
      "loss:  0.007632856711424744\n",
      "117\n",
      "loss:  0.008226637088992968\n",
      "118\n",
      "loss:  0.007519769671000059\n",
      "119\n",
      "loss:  0.008024109708895421\n",
      "120\n",
      "loss:  0.007588282105076615\n",
      "121\n",
      "loss:  0.007801200007949064\n",
      "122\n",
      "loss:  0.007728133207548268\n",
      "123\n",
      "loss:  0.00736127425465759\n",
      "124\n",
      "loss:  0.007889753319457916\n",
      "125\n",
      "loss:  0.008433252764980524\n",
      "126\n",
      "loss:  0.0074064057375863935\n",
      "127\n",
      "loss:  0.007689359783312932\n",
      "128\n",
      "loss:  0.006754013007710775\n",
      "129\n",
      "loss:  0.007476199770212952\n",
      "130\n",
      "loss:  0.006109484506258691\n",
      "131\n",
      "loss:  0.0062965262905478915\n",
      "132\n",
      "loss:  0.00623647283755226\n",
      "133\n",
      "loss:  0.007158584007859651\n",
      "134\n",
      "loss:  0.0067478438540497345\n",
      "135\n",
      "loss:  0.008064237988493308\n",
      "136\n",
      "loss:  0.007476689542396454\n",
      "137\n",
      "loss:  0.006269478781853787\n",
      "138\n",
      "loss:  0.007535144571398768\n",
      "139\n",
      "loss:  0.007005019387164749\n",
      "140\n",
      "loss:  0.006186996795230253\n",
      "141\n",
      "loss:  0.006431644465369008\n",
      "142\n",
      "loss:  0.006654921109135785\n",
      "143\n",
      "loss:  0.007175697905329953\n",
      "144\n",
      "loss:  0.006586489803997295\n",
      "145\n",
      "loss:  0.007185080665930476\n",
      "146\n",
      "loss:  0.006397715383970817\n",
      "147\n",
      "loss:  0.0057023393129091654\n",
      "148\n",
      "loss:  0.0072468071121580555\n",
      "149\n",
      "loss:  0.007107119792435187\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.0005, momentum=0.9, weight_decay=5e-4)\n",
    "print(\"training...\")\n",
    "print(len(train_dataset))\n",
    "net.train()\n",
    "for epoch in range(150):  # loop over the dataset multiple times\n",
    "    print(epoch)\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        #print(np.unique(data[2], return_counts=True))\n",
    "        #print(data[2])\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(x1, x2)\n",
    "        outputs = outputs.reshape((outputs.shape[0]))\n",
    "        #labels = labels.reshape((labels.shape[0], 1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "    print(\"loss: \", running_loss/step)\n",
    "    step = 0\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy as scipy_entropy\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "probs = torch.zeros([len(test_dataset), 2]).to(device)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    evaluated_instances = 0\n",
    "    for data in test_loader:\n",
    "        x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(x1, x2)\n",
    "        pred = nn.Sigmoid()(outputs)\n",
    "        #print(pred)\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred = pred.reshape(pred.shape[0])\n",
    "        prob = torch.Tensor([1-pred, pred]).T.to(device)\n",
    "        start_slice = evaluated_instances\n",
    "        end_slice = start_slice + x1.shape[0]\n",
    "        probs[start_slice:end_slice] = prob\n",
    "        evaluated_instances = end_slice\n",
    "        #entropy = scipy_entropy(prob)\n",
    "        #pred[pred >= 0.5] = 1\n",
    "        #pred[pred < 0.5] = 0\n",
    "        #print(torch.max(pred.data, 1)[1])\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        #_, predicted = torch.max(pred.data, 1)\n",
    "probs = probs.cpu().numpy()\n",
    "preds = np.argmax(probs, axis=1)\n",
    "entropys = scipy_entropy(probs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is on right device and is in TRAIN mode.\n",
    "# Train mode is needed to activate randomness in dropout modules.\n",
    "n_drop = 10\n",
    "net.train()\n",
    "# Create a tensor to hold probabilities\n",
    "num_classes = 2\n",
    "probs_mc = torch.zeros([len(test_dataset), 2]).to(device)\n",
    "# Create a dataloader object to load the dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Repeat n_drop number of times to obtain n_drop dropout samples per data instance\n",
    "    for i in range(n_drop):\n",
    "        evaluated_instances = 0\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            # Calculate softmax (probabilities) of predictions\n",
    "            x1, x2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "            outputs = net(x1, x2)\n",
    "            pred = nn.Sigmoid()(outputs)\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred = pred.reshape(pred.shape[0])\n",
    "            prob = torch.Tensor([1-pred, pred]).T.to(device)\n",
    "        \n",
    "            # Accumulate the calculated batch of probabilities into the tensor to return\n",
    "            start_slice = evaluated_instances\n",
    "            end_slice = start_slice + x1.shape[0]\n",
    "            probs_mc[start_slice:end_slice] += prob\n",
    "            evaluated_instances = end_slice\n",
    "\n",
    "# Divide through by n_drop to get average prob.\n",
    "probs_mc /= n_drop\n",
    "probs_mc = probs_mc.cpu().numpy()\n",
    "preds_mc = np.argmax(probs_mc, axis=1)\n",
    "entropys_mc = scipy_entropy(probs_mc.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = 0.01\n",
    "threshold = 1e-13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(entropys <= threshold)[0]\n",
    "preds_new = preds[inds]\n",
    "y_test_pairs_new = y_test_pairs[inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(entropys_mc <= threshold)[0]\n",
    "preds_new_mc = preds_mc[inds]\n",
    "y_test_pairs_new_mc = y_test_pairs[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00      9407\n",
      "         1.0       0.94      0.99      0.96       836\n",
      "\n",
      "    accuracy                           0.99     10243\n",
      "   macro avg       0.97      0.99      0.98     10243\n",
      "weighted avg       0.99      0.99      0.99     10243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_pairs_new, preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      4182\n",
      "         1.0       1.00      1.00      1.00       220\n",
      "\n",
      "    accuracy                           1.00      4402\n",
      "   macro avg       1.00      1.00      1.00      4402\n",
      "weighted avg       1.00      1.00      1.00      4402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_pairs_new_mc, preds_new_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
